{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Character-Level RNN Text Generator\n",
        "\n",
        "A vanilla RNN implementation from scratch for character-level text generation. Learn RNN fundamentals by building a model that generates text one character at a time.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Input (char) → One-Hot → RNN Cell → Softmax → Output (next char)\n",
        "                           ↓\n",
        "                    Hidden State (memory)\n",
        "```\n",
        "\n",
        "### RNN Cell Equations\n",
        "\n",
        "```\n",
        "h_t = tanh(W_xh · x_t + W_hh · h_{t-1} + b_h)\n",
        "y_t = W_hy · h_t + b_y\n",
        "p_t = softmax(y_t)\n",
        "```\n",
        "\n",
        "**Where:**\n",
        "- `x_t` = current character (one-hot encoded)\n",
        "- `h_t` = hidden state (memory)\n",
        "- `W_xh, W_hh, W_hy` = weight matrices\n",
        "- `b_h, b_y` = biases\n",
        "\n",
        "### Parameters\n",
        "\n",
        "For vocab size `V=50`, hidden size `H=128`:\n",
        "- W_xh: 128 × 50\n",
        "- W_hh: 128 × 128  \n",
        "- W_hy: 50 × 128\n",
        "- **Total: ~29k parameters**\n",
        "\n",
        "## How It Works\n",
        "\n",
        "**Training:**\n",
        "1. Convert text to character indices\n",
        "2. Create input-target pairs (shifted by 1 char)\n",
        "3. Forward pass through sequence\n",
        "4. Backpropagation Through Time (BPTT)\n",
        "5. Update weights with gradient descent\n",
        "\n",
        "**Generation:**\n",
        "1. Start with seed text\n",
        "2. Get probability distribution for next char\n",
        "3. Sample using temperature\n",
        "4. Append and repeat\n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "```python\n",
        "hidden_size = 128        \n",
        "seq_length = 25          \n",
        "learning_rate = 0.01     \n",
        "temperature = 0.7        \n",
        "```\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "rnn = CharRNN(vocab_size, hidden_size=128)\n",
        "rnn.train(text, epochs=100)\n",
        "output = rnn.generate(seed=\"To be\", length=200, temp=0.7)\n",
        "```\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "Sequence modeling with hidden states  \n",
        "Backpropagation through time  \n",
        "Gradient clipping (prevents exploding gradients)  \n",
        "Temperature-based sampling  \n",
        "\n",
        "## Extensions\n",
        "\n",
        "- Implement LSTM cells\n",
        "- Multi-layer RNN\n",
        "- Train on code/poetry/different styles\n",
        "- Add beam search for generation\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Karpathy's RNN Post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "- [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lwTlInoaeaCx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c75mcDvieZYX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "\n",
        "In Paris, urban farmers are trying a soil-free approach to agriculture that uses less\n",
        "space and fewer resources. Could it help cities face the threats to our food supplies?\n",
        "On top of a striking new exhibition hall in southern Paris, the world's largest urban rooftop farm\n",
        "has started to bear fruit. Strawberries that are small, intensely flavoured and resplendently red\n",
        "sprout abundantly from large plastic tubes. Peer inside and you see the tubes are completely\n",
        "hollow, the roots of dozens of strawberry plants dangling down inside them. From identical\n",
        "vertical tubes nearby burst row upon row of lettuces; near those are aromatic herbs, such as basil,\n",
        "sage and peppermint. Opposite, in narrow, horizontal trays packed not with soil but with coconut\n",
        "fibre, grow cherry tomatoes, shiny aubergines and brightly coloured chards.\n",
        "Pascal Hardy, an engineer and sustainable development consultant, began experimenting with\n",
        "vertical farming and aeroponic growing towers - as the soil-free plastic tubes are known - on\n",
        "his Paris apartment block roof five years ago. The urban rooftop space above the exhibition hall\n",
        "is somewhat bigger: 14,000 square metres and almost exactly the size of a couple of football\n",
        "pitches. Already, the team of young urban farmers who tend it have picked, in one day, 3,000\n",
        "lettuces and 150 punnets of strawberries. When the remaining two thirds of the vast open area\n",
        "are in production, 20 staff will harvest up to 1,000 kg of perhaps 35 different varieties of fruit\n",
        "and vegetables, every day. 'We're not ever, obviously, going to feed the whole city this way,\n",
        "cautions Hardy. 'In the urban environment you're working with very significant practical\n",
        "constraints, clearly, on what you can do and where. But if enough unused space can be developed\n",
        "like this, there's no reason why you shouldn't eventually target maybe between 5% and 10%\n",
        "of consumption.'\n",
        "Perhaps most significantly, however, this is a real-life showcase for the work of Hardy's\n",
        "flourishing urban agriculture consultancy, Agripolis, which is currently fielding enquiries from\n",
        "around the world to design, build and equip a new breed of soil-free inner-city farm. 'The\n",
        "method's advantages are many,' he says. 'First, I don't much like the fact that most of the fruit\n",
        "and vegetables we eat have been treated with something like 17 different pesticides, or that\n",
        "the intensive farming techniques that produced them are such huge generators of greenhouse\n",
        "16\n",
        "Readading\n",
        "gases. I don't much like the fact, either, that they've travelled an average of 2,000 refrigerated\n",
        "kilometres to my plate, that their quality is so poor, because the varieties are selected for their\n",
        "capacity to withstand such substantial journeys, or that 80% ofthe price I pay goes to wholesalers\n",
        "and transport companies, not the producers.\n",
        "Produce grown using this soil-free method, on the other hand - which relies solely on a small\n",
        "quantity of water, enriched with organic nutrients, pumped around a closed circuit of pipes,\n",
        "towers and trays - is 'produced up here, and sold locally, just down there. It barely travels at all,'\n",
        "Hardy says. 'You can select crop varieties for their flavour, not their resistance to the transport\n",
        "and storage chain, and you can pick them when they're really at their best, and not before.' No\n",
        "soil is exhausted, and the water that gently showers the plants' roots every 12 minutes is recycled,\n",
        "so the method uses 90% less water than a classic intensive farm for the same yield.\n",
        "Urban farming is not, of course, a new phenomenon. Inner-city agriculture is booming from\n",
        "Shanghai to Detroit and Tokyo to Bangkok. Strawberries are being grown in disused shipping\n",
        "containers, mushrooms in underground carparks. Aeroponic farming, he says, is 'virtuous'. The\n",
        "equipment weighs little, can be installed on almost any flat surface and is cheap to buy: roughly\n",
        "€100 to €150 per square metre. It is cheap to run, too, consuming a tiny fraction of the electricity\n",
        "used by some techniques.\n",
        "Produce grown this way typically sells at prices that, while generally higher than those of classic\n",
        "intensive agriculture, are lower than soil-based organic growers. There are limits to what farmers\n",
        "can grow this way, of course, and much of the produce is suited to the summer months. 'Root\n",
        "vegetables we cannot do, at least not yet,' he says. 'Radishes are OK, but carrots, potatoes, that\n",
        "kind of thing - the roots are simply too long. Fruit trees are obviously not an option. And beans\n",
        "tend to take up a lot of space for not much return.' Nevertheless, urban farming of the kind\n",
        "being practised in Paris is one part of a bigger and fast-changing picture that is bringing food\n",
        "production closer to our lives\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_cN9bKLye7ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "print(f\"Text length: {len(text)}\")\n",
        "print(f\"Cherectors: {repr(' ' .join(chars))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p1jR9jEhZpd",
        "outputId": "ab5ae725-1fc4-4551-9182-6ed1063ff9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 64\n",
            "Text length: 4671\n",
            "Cherectors: \"\\n   % ' , - . 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D F H I K N O P R S T U W Y a b c d e f g h i j k l m n o p q r s t u v w x y z €\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN:\n",
        "    def __init__(self, vocab_size, hidden_size=80, seq_lenght=25, lr=0.1):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.seq_length = seq_lenght\n",
        "        self.lr = lr\n",
        "\n",
        "        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Wxh = np.random.randn(vocab_size, hidden_size) * 0.01\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "        self.mWxh, self.mWhh, self.mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
        "        self.mbh, self.mby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
        "\n",
        "    def loss_and_gradients(self, input, targets, hprev):\n",
        "      xs, hs, ys, ps = {}, {}, {}, {}\n",
        "      hs[-1] = np.copy(hprev)\n",
        "      loss = 0\n",
        "\n",
        "      for t in range(len(inputs)):\n",
        "            xs[t] = np.zeros((self.vocab_size, 1))\n",
        "            xs[t][inputs[t]] = 1\n",
        "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh)\n",
        "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
        "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
        "            loss += -np.log(ps[t][targets[t], 0])\n",
        "\n",
        "      dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
        "      dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
        "      dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "      for t in reversed(range(len(inputs))):\n",
        "            dy = np.copy(ps[t])\n",
        "            dy[targets[t]] -= 1\n",
        "            dWhy += np.dot(dy, hs[t].T)\n",
        "            dby += dy\n",
        "            dh = np.dot(self.Why.T, dy) + dhnext\n",
        "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
        "            dbh += dhraw\n",
        "            dWxh += np.dot(dhraw, xs[t].T)\n",
        "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "            dhnext = np.dot(self.Whh.T, dhraw)\n",
        "\n",
        "      for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "      return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
        "\n",
        "\n",
        "    def sample(self, h, seed_ix, n):\n",
        "        x = np.zeros((self.vocab_size, 1))\n",
        "        x[seed_ix] = 1\n",
        "        ixes = []\n",
        "        for t in range(n):\n",
        "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
        "            y = np.dot(self.Why, h) + self.by\n",
        "            p = np.exp(y) / np.sum(np.exp(y))\n",
        "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
        "            x = np.zeros((self.vocab_size, 1))\n",
        "            x[ix] = 1\n",
        "            ixes.append(ix)\n",
        "        return ixes"
      ],
      "metadata": {
        "id": "sH72xVLGiIPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN:\n",
        "    def __init__(self, vocab_size, hidden_size=80, seq_length=25, lr=0.1):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.seq_length = seq_length # Corrected typo\n",
        "        self.lr = lr\n",
        "\n",
        "        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "        self.mWxh, self.mWhh, self.mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
        "        self.mbh, self.mby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
        "\n",
        "    def loss_and_gradients(self, input, targets, hprev):\n",
        "      xs, hs, ys, ps = {}, {}, {}, {}\n",
        "      hs[-1] = np.copy(hprev)\n",
        "      loss = 0\n",
        "\n",
        "      for t in range(len(inputs)):\n",
        "            xs[t] = np.zeros((self.vocab_size, 1))\n",
        "            xs[t][inputs[t]] = 1\n",
        "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh)\n",
        "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
        "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
        "            loss += -np.log(ps[t][targets[t], 0])\n",
        "\n",
        "      dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
        "      dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
        "      dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "      for t in reversed(range(len(inputs))):\n",
        "            dy = np.copy(ps[t])\n",
        "            dy[targets[t]] -= 1\n",
        "            dWhy += np.dot(dy, hs[t].T)\n",
        "            dby += dy\n",
        "            dh = np.dot(self.Why.T, dy) + dhnext\n",
        "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
        "            dbh += dhraw\n",
        "            dWxh += np.dot(dhraw, xs[t].T)\n",
        "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "            dhnext = np.dot(self.Whh.T, dhraw)\n",
        "\n",
        "      for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "      return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
        "\n",
        "\n",
        "    def sample(self, h, seed_ix, n):\n",
        "        x = np.zeros((self.vocab_size, 1))\n",
        "        x[seed_ix] = 1\n",
        "        ixes = []\n",
        "        for t in range(n):\n",
        "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
        "            y = np.dot(self.Why, h) + self.by\n",
        "            p = np.exp(y) / np.sum(np.exp(y))\n",
        "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
        "            x = np.zeros((self.vocab_size, 1))\n",
        "            x[ix] = 1\n",
        "            ixes.append(ix)\n",
        "        return ixes\n",
        "\n",
        "data = [char_to_idx[ch] for ch in text]\n",
        "rnn = CharRNN(vocab_size, hidden_size=50, seq_length=25)\n",
        "\n",
        "n, p = 0, 0\n",
        "hprev = np.zeros((rnn.hidden_size, 1))\n",
        "smooth_loss = -np.log(1.0/vocab_size) * rnn.seq_length\n",
        "\n",
        "print(\"Training started...\\n\")\n",
        "\n",
        "for iteration in range(5000):\n",
        "    if p + rnn.seq_length + 1 >= len(data) or n == 0:\n",
        "        hprev = np.zeros((rnn.hidden_size, 1))\n",
        "        p = 0\n",
        "\n",
        "    inputs = data[p:p+rnn.seq_length]\n",
        "    targets = data[p+1:p+rnn.seq_length+1]\n",
        "\n",
        "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = rnn.loss_and_gradients(inputs, targets, hprev)\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "    for param, dparam, mem in zip([rnn.Wxh, rnn.Whh, rnn.Why, rnn.bh, rnn.by],\n",
        "                                   [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                   [rnn.mWxh, rnn.mWhh, rnn.mWhy, rnn.mbh, rnn.mby]):\n",
        "        mem += dparam * dparam\n",
        "        param += -rnn.lr * dparam / np.sqrt(mem + 1e-8)\n",
        "\n",
        "    p += rnn.seq_length\n",
        "    n += 1\n",
        "\n",
        "    if n % 500 == 0:\n",
        "        print(f'Iteration {n}, Loss: {smooth_loss:.4f}')\n",
        "        sample_ix = rnn.sample(hprev, inputs[0], 200)\n",
        "        txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
        "        print(f'Generated:\\n{txt}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwxVf0q8iGhn",
        "outputId": "e9aeaf9f-c0bc-4fb9-fdc1-5946a24feb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "\n",
            "Iteration 500, Loss: 93.8700\n",
            "Generated:\n",
            " fuu0rit Fe He ree, it pP tes graituCi il,  9epg dan Yd e tyyimfe tohn.  ang th are :esryet r arent dr pro paralume i%srs hoysrr sarile \n",
            "\n",
            "a\n",
            "€stonto wa. at honsnpf k t\n",
            "dacdxceern tarosecph nds fren nit\n",
            "\n",
            "Iteration 1000, Loss: 83.2352\n",
            "Generated:\n",
            "ns'wbale det\n",
            "as sory gain, tUnouwe'dd ch tereid to, baf oo tthicenicors\n",
            "sherre wot pocita euphe ka r€miblglSd om sarin fetid mopvyrl.d ?pand.uYitu arBice agesy €nmpouare ssqin thand thd Sraddw\n",
            "or. tri\n",
            "\n",
            "Iteration 1500, Loss: 74.8722\n",
            "Generated:\n",
            "zowisy he, ant srik focint, lins ont uradite wrous xhe ucild parotis..\n",
            "16qberolturering he Parang pit is. bent ficree te tes lees thacsst, ores muanlants not\n",
            "ke tacrly rederrenb ive wangs. 1Con parbei\n",
            "\n",
            "Iteration 2000, Loss: 69.0298\n",
            "Generated:\n",
            "ltouts rethy to pody s evoull froicp shelives caverdats eroules. Is the tesProags shis weithec trvotuce lame terine trithirves, theigoblns, frbath oly\n",
            "its sis r0in\n",
            "fyrt anme caru oed paraf thiss halt \n",
            "\n",
            "Iteration 2500, Loss: 64.6908\n",
            "Generated:\n",
            "the alot sow, She  tarmend\n",
            "20\n",
            "s cot agitrom\n",
            "thunticuy, ego sof the tuacswrathe\n",
            "usm andan6\n",
            "Uropars.\n",
            "Prealoty exuce of carost wucucelasely, -undod cuetinnt ind' en 150? Oriss arman uptrglot\n",
            "oucercovan i\n",
            "\n",
            "Iteration 3000, Loss: 61.1291\n",
            "Generated:\n",
            "equaat bes fire -o atan sur's trod to, ac outot, lof y,ls aod flealllop tid thare space ppas suce ceveroftricuit angroolings ufanel in harucr sayt of sines.\n",
            "Khe wan therm thalt wewn farande lenttaly t\n",
            "\n",
            "Iteration 3500, Loss: 58.4618\n",
            "Generated:\n",
            "ply arsile veo thatt tuce ke fory lot on tysaner\n",
            "eit the tod ifrowif ungaree, efan baticite nos, 15% gares mhepe jouly leyt of arople souther iehavere shise ind is avansaces mewantivreantiertb ofich i\n",
            "\n",
            "Iteration 4000, Loss: 56.2584\n",
            "Generated:\n",
            " twube, murming and the thas,\n",
            "broit mezethe the woalg share sith thad'an in doaldvastin5?\n",
            "CUwhe onl ss boomesll, inture lysast ous ho god depese exomicow, \n",
            "Usent's, cand that\n",
            "en ind wo poistag ger:is \n",
            "\n",
            "Iteration 4500, Loss: 54.8296\n",
            "Generated:\n",
            " thet.\n",
            "Alkes'se tubher: sith tuw bigrbant purdunge a cata in as rethades the sher muct gemion. OKe's vot the istiend celtucbers way\n",
            "Tas. thike thet ir thato\n",
            "kones\n",
            "lot d argert tuuse pealls and thes ae\n",
            "\n",
            "Iteration 5000, Loss: 53.4520\n",
            "Generated:\n",
            "lnmit samp werresusse the wouch ket farmicaddantat grnyant, y, oulfars, agreinshal \n",
            "ise ch primen, nucn alt evare socld sofroingonsp ons?\n",
            "Urdents suop ouy'ng thois weva ticiet lermift, tsmate poupic t\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL GENERATION\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "h = np.zeros((rnn.hidden_size, 1))\n",
        "seed = \"organic:\"\n",
        "for ch in seed:\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[char_to_idx[ch]] = 1\n",
        "    h = np.tanh(np.dot(rnn.Wxh, x) + np.dot(rnn.Whh, h) + rnn.bh)\n",
        "\n",
        "sample_ix = rnn.sample(h, char_to_idx[seed[-1]], 300)\n",
        "generated = seed + ''.join(idx_to_char[ix] for ix in sample_ix)\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmA-kBuUkNbu",
        "outputId": "8a008282-672f-4d38-a099-68fc4ef9529a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL GENERATION\n",
            "============================================================\n",
            "\n",
            "organic:, bo agy then in surs. O1% ar: altith size seasisuly use rete rnwuleso then ticantan suwithirienbnarse y gerriche a8tis thasuns. 'ThI whuc, aet ains. Ann pethise reoute sooll\n",
            "toum monndund aled tiou ontl,ost0: . yoty wertke to of fretand\n",
            "ngan bigpan dultuzdu bibag of chalpicble Inon\n",
            "is\n",
            "of gier, fart\n"
          ]
        }
      ]
    }
  ]
}