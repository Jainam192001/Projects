{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This project demonstrates a basic PyTorch workflow using the FashionMNIST dataset.\n",
        "It covers data loading with DataLoader (batch size = 64), model creation using fully connected layers,\n",
        "training with CrossEntropyLoss and SGD optimizer, and evaluation on test data.\n",
        "The goal is to provide a clear, beginner-friendly example of building and training a neural network in PyTorch.\n"
      ],
      "metadata": {
        "id": "Z-NCJ_cztLLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "d8THc1U9es9y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVyc3_41fuoI",
        "outputId": "1a3e2520-329b-4945-837c-45bdb28d2fe0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 20.6MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 306kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.66MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"shape of y: {y.shape}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er4hVMX_gdcd",
        "outputId": "7437ff31-bc6e-4732-83fd-c636e95cca54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "shape of y: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "     def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "     def forward(self, x):\n",
        "      x = self.flatten(x)\n",
        "      logits = self.linear_relu_stack(x)\n",
        "      return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lkz0DhniYi4",
        "outputId": "ffdfc9ad-0db7-433d-9d35-502d8e3feed6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "oCrJMDamlYco"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "iSdrRjzvlZvL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"welldone Patel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5xT6xhgn7GQ",
        "outputId": "10056816-19c2-445d-9c97-92bbb07120ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.307902  [   64/60000]\n",
            "loss: 2.286206  [ 6464/60000]\n",
            "loss: 2.266278  [12864/60000]\n",
            "loss: 2.255149  [19264/60000]\n",
            "loss: 2.247071  [25664/60000]\n",
            "loss: 2.228884  [32064/60000]\n",
            "loss: 2.219669  [38464/60000]\n",
            "loss: 2.193134  [44864/60000]\n",
            "loss: 2.183137  [51264/60000]\n",
            "loss: 2.153675  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.5%, Avg loss: 2.150766 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.165136  [   64/60000]\n",
            "loss: 2.149737  [ 6464/60000]\n",
            "loss: 2.093729  [12864/60000]\n",
            "loss: 2.103497  [19264/60000]\n",
            "loss: 2.055099  [25664/60000]\n",
            "loss: 2.013652  [32064/60000]\n",
            "loss: 2.010032  [38464/60000]\n",
            "loss: 1.942798  [44864/60000]\n",
            "loss: 1.936148  [51264/60000]\n",
            "loss: 1.864876  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.3%, Avg loss: 1.868682 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.907151  [   64/60000]\n",
            "loss: 1.872995  [ 6464/60000]\n",
            "loss: 1.756315  [12864/60000]\n",
            "loss: 1.786600  [19264/60000]\n",
            "loss: 1.678272  [25664/60000]\n",
            "loss: 1.653199  [32064/60000]\n",
            "loss: 1.642493  [38464/60000]\n",
            "loss: 1.558760  [44864/60000]\n",
            "loss: 1.575731  [51264/60000]\n",
            "loss: 1.472054  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.9%, Avg loss: 1.496385 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.570494  [   64/60000]\n",
            "loss: 1.533239  [ 6464/60000]\n",
            "loss: 1.384531  [12864/60000]\n",
            "loss: 1.447941  [19264/60000]\n",
            "loss: 1.334476  [25664/60000]\n",
            "loss: 1.349292  [32064/60000]\n",
            "loss: 1.345488  [38464/60000]\n",
            "loss: 1.276171  [44864/60000]\n",
            "loss: 1.310979  [51264/60000]\n",
            "loss: 1.216722  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.242274 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.325686  [   64/60000]\n",
            "loss: 1.303713  [ 6464/60000]\n",
            "loss: 1.136673  [12864/60000]\n",
            "loss: 1.236033  [19264/60000]\n",
            "loss: 1.118996  [25664/60000]\n",
            "loss: 1.156278  [32064/60000]\n",
            "loss: 1.166579  [38464/60000]\n",
            "loss: 1.102491  [44864/60000]\n",
            "loss: 1.146446  [51264/60000]\n",
            "loss: 1.067298  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 1.085932 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.162318  [   64/60000]\n",
            "loss: 1.159875  [ 6464/60000]\n",
            "loss: 0.974927  [12864/60000]\n",
            "loss: 1.104240  [19264/60000]\n",
            "loss: 0.986790  [25664/60000]\n",
            "loss: 1.026531  [32064/60000]\n",
            "loss: 1.054762  [38464/60000]\n",
            "loss: 0.990146  [44864/60000]\n",
            "loss: 1.037469  [51264/60000]\n",
            "loss: 0.972868  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.4%, Avg loss: 0.984271 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.046291  [   64/60000]\n",
            "loss: 1.065810  [ 6464/60000]\n",
            "loss: 0.863836  [12864/60000]\n",
            "loss: 1.015797  [19264/60000]\n",
            "loss: 0.903576  [25664/60000]\n",
            "loss: 0.934082  [32064/60000]\n",
            "loss: 0.980680  [38464/60000]\n",
            "loss: 0.915122  [44864/60000]\n",
            "loss: 0.961151  [51264/60000]\n",
            "loss: 0.908676  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.6%, Avg loss: 0.913963 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.959411  [   64/60000]\n",
            "loss: 0.999534  [ 6464/60000]\n",
            "loss: 0.783500  [12864/60000]\n",
            "loss: 0.952396  [19264/60000]\n",
            "loss: 0.847894  [25664/60000]\n",
            "loss: 0.865423  [32064/60000]\n",
            "loss: 0.927919  [38464/60000]\n",
            "loss: 0.863578  [44864/60000]\n",
            "loss: 0.905589  [51264/60000]\n",
            "loss: 0.861623  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.0%, Avg loss: 0.862830 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.892178  [   64/60000]\n",
            "loss: 0.949478  [ 6464/60000]\n",
            "loss: 0.723586  [12864/60000]\n",
            "loss: 0.904741  [19264/60000]\n",
            "loss: 0.808499  [25664/60000]\n",
            "loss: 0.813712  [32064/60000]\n",
            "loss: 0.888071  [38464/60000]\n",
            "loss: 0.826702  [44864/60000]\n",
            "loss: 0.864161  [51264/60000]\n",
            "loss: 0.825664  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.0%, Avg loss: 0.824059 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.838712  [   64/60000]\n",
            "loss: 0.909258  [ 6464/60000]\n",
            "loss: 0.677457  [12864/60000]\n",
            "loss: 0.868161  [19264/60000]\n",
            "loss: 0.778861  [25664/60000]\n",
            "loss: 0.774034  [32064/60000]\n",
            "loss: 0.855991  [38464/60000]\n",
            "loss: 0.799178  [44864/60000]\n",
            "loss: 0.831792  [51264/60000]\n",
            "loss: 0.796679  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 0.793323 \n",
            "\n",
            "welldone Patel\n"
          ]
        }
      ]
    }
  ]
}