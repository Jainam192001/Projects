# -*- coding: utf-8 -*-
"""Untitled33.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oDiNDU0BvH9FbqnLXVbOu-qb3zt2mra_

QUE 1 : How much Participetion happened in each and every year?
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv('/content/drive/MyDrive/Toronto_hourly_precipitation_1982_2023 Raw Data (1).csv')

print(df.info())

print(df.head())

"""We need to convert data actual date and time format."""

# Convert 'datetime' column to actual datetime format
df['datetime'] = pd.to_datetime(df['datetime'])

# Extract month and year as new columns
df['month'] = df['datetime'].dt.month
df['year'] = df['datetime'].dt.year

# Display first few rows to verify
print(df[['datetime', 'month', 'year']].head(20))
print(df[['datetime', 'month', 'year']].tail(20))

"""Find Avg Toatal, Average and variance of precipitation"""

df['precip'] = pd.to_numeric(df['precip'], errors='coerce').fillna(0)

# Aggregate precipitation per month
monthly_precip = df.groupby('month')['precip'].agg(
    total_precipitation='sum',  # Total precipitation per month
    avg_precipitation='mean',   # Average hourly precipitation per month
    variance_precipitation='var' # Variance of precipitation per month
).reset_index()

# Display results
print(monthly_precip)

sns.set_theme(style="whitegrid")

# Create bar plot for total precipitation per month
plt.figure(figsize=(10, 5))
sns.barplot(x=monthly_precip['month'], y=monthly_precip['total_precipitation'], palette="Blues_r")
plt.xlabel("Month")
plt.ylabel("Total Precipitation (mm)")
plt.title("Total Precipitation by Month in Toronto (1982-2023)")
plt.xticks(range(12), ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])
plt.show()

# Create box plot for precipitation distribution per month
plt.figure(figsize=(10, 5))
sns.boxplot(x=df["month"], y=df["precip"], palette="coolwarm")
plt.xlabel("Month")
plt.ylabel("Precipitation (mm)")
plt.title("Monthly Precipitation Distribution in Toronto (1982-2023)")
plt.xticks(range(12), ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"])
plt.show()

# Select relevant features for clustering (same as before)
features = ['total_precipitation', 'avg_precipitation', 'variance_precipitation']
scaler = StandardScaler()
monthly_precip_scaled = scaler.fit_transform(monthly_precip[features])

# Apply K-Means with k=5
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
monthly_precip['cluster'] = kmeans.fit_predict(monthly_precip_scaled)

# Sort clusters by mean total precipitation to assign meaningful labels
cluster_means = monthly_precip.groupby("cluster")["total_precipitation"].mean().sort_values()

# Create more granular labels based on precipitation levels
precip_levels = ["Very low", "Low", "Medium", "High", "Very High"]
cluster_mapping = {cluster: level for cluster, level in zip(cluster_means.index, precip_levels)}
monthly_precip['precip_category'] = monthly_precip['cluster'].map(cluster_mapping)

# Display clustered data
print(monthly_precip[['month', 'total_precipitation', 'precip_category']].sort_values('total_precipitation', ascending=False))

# Print cluster statistics
print("\nCluster Statistics:")
print(monthly_precip.groupby('precip_category')['total_precipitation'].describe())

"""Now just implement hyperparameter tuning by taking value of K = 2 and K = 3"""

# Select relevant features for clustering (same as before)
features = ['total_precipitation', 'avg_precipitation', 'variance_precipitation']

# Standardize the features
scaler = StandardScaler()
monthly_precip_scaled = scaler.fit_transform(monthly_precip[features])

# Apply K-Means with k=3
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
monthly_precip['cluster'] = kmeans.fit_predict(monthly_precip_scaled)

# Sort clusters by mean total precipitation to assign meaningful labels
cluster_means = monthly_precip.groupby("cluster")["total_precipitation"].mean().sort_values()

# Create more granular labels based on precipitation levels
precip_levels = ["Very Low", "Low", "Medium", "High", "Very High"]
cluster_mapping = {cluster: level for cluster, level in zip(cluster_means.index, precip_levels)}
monthly_precip['precip_category'] = monthly_precip['cluster'].map(cluster_mapping)

# Display clustered data
print(monthly_precip[['month', 'total_precipitation', 'precip_category']].sort_values('total_precipitation', ascending=False))

# Print cluster statistics
print("\nCluster Statistics:")
print(monthly_precip.groupby('precip_category')['total_precipitation'].describe())

# Select relevant features for clustering (same as before)
features = ['total_precipitation', 'avg_precipitation', 'variance_precipitation']

# Standardize the features
scaler = StandardScaler()
monthly_precip_scaled = scaler.fit_transform(monthly_precip[features])

# Apply K-Means with k=2
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
monthly_precip['cluster'] = kmeans.fit_predict(monthly_precip_scaled)

# Sort clusters by mean total precipitation to assign meaningful labels
cluster_means = monthly_precip.groupby("cluster")["total_precipitation"].mean().sort_values()

# Create more granular labels based on precipitation levels
precip_levels = ["Very Low", "Low", "Medium", "High", "Very High"]
cluster_mapping = {cluster: level for cluster, level in zip(cluster_means.index, precip_levels)}
monthly_precip['precip_category'] = monthly_precip['cluster'].map(cluster_mapping)

# Display clustered data
print(monthly_precip[['month', 'total_precipitation', 'precip_category']].sort_values('total_precipitation', ascending=False))

# Print cluster statistics
print("\nCluster Statistics:")
print(monthly_precip.groupby('precip_category')['total_precipitation'].describe())

"""According to KNN result of this question is on and average Mid time of year has most rainy months as compared to begin and enf of the year months in Toronto

This silhouette score shows the accuracy. Lower the score Higher the accurecy and in this case model score is 0.461.
"""

from sklearn.metrics import silhouette_score
sil_score = silhouette_score(monthly_precip_scaled, monthly_precip['cluster'])
print(f"Silhouette Score: {sil_score:.3f}")

"""Question 2 : Has there been an increase in extreme precipitation events over the years?

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression  # Better for classification
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, roc_auc_score, confusion_matrix,
                            RocCurveDisplay, PrecisionRecallDisplay)

# Load and prepare data
df = pd.read_csv('/content/drive/MyDrive/Toronto_hourly_precipitation_1982_2023 Raw Data (1).csv',
                 parse_dates=['datetime'])
df['year'] = df['datetime'].dt.year

# Feature Engineering
df['temp_change'] = df['temp'].diff(1)  # 1-hour temperature delta
df['pressure_change'] = df['sealevelpressure'].diff(1)

# Define extreme precipitation (top 5% of hourly values)
extreme_threshold = df['precip'].quantile(0.95)
df['extreme_precip'] = (df['precip'] >= extreme_threshold).astype(int)

# Select relevant features
features = [
    'temp',
    'temp_change',
    'humidity',
    'windspeed',
    'sealevelpressure',
    'pressure_change',
    'cloudcover',
    'precipprob'
]

"""Need to do some data cleaning. Based on question we need to remove null values."""

# Handle missing values
df[features] = df[features].fillna(df[features].median())

# Normalize features (MinMax for probabilistic interpretation)
scaler = MinMaxScaler()
X = scaler.fit_transform(df[features])
y = df['extreme_precip']

# Split data (stratified to preserve class imbalance)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""Now implement the logistic regression as our data is binary so it is very good approch to use logistic regression"""

# Use Logistic Regression (because of binary classification)
model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for AUC

# Evaluation Metrics
print("Model Performance")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall: {recall_score(y_test, y_pred):.3f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.3f}")
print(f"ROC AUC: {roc_auc_score(y_test, y_prob):.3f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Normal', 'Extreme'],
            yticklabels=['Normal', 'Extreme'])
plt.title("Confusion Matrix")
plt.show()

# ROC Curve
RocCurveDisplay.from_estimator(model, X_test, y_test)
plt.title("ROC Curve")
plt.show()

# Feature Importance
importance = pd.DataFrame({
    'Feature': features,
    'Coefficient': model.coef_[0]
}).sort_values('Coefficient', key=abs, ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Coefficient', y='Feature', data=importance)
plt.title("Feature Importance (Logistic Regression Coefficients)")
plt.show()

# Trend Analysis (Extreme Events Over Time)
plt.figure(figsize=(12, 6))
df.groupby('year')['extreme_precip'].mean().plot(kind='bar')
plt.title("Percentage of Extreme Precipitation Events by Year")
plt.ylabel("Proportion of Extreme Events")
plt.show()

"""Accuracy: 0.983 Precision: 0.744 Recall: 0.993 F1 Score: 0.851 ROC AUC: 0.993

SO this logistic regression gave us above accuracy result which denoted that model is train on good scale

Que 3:How does hourly precipitation correlate with temperature changes?

So, Here we are going to use MLR(MULTIPLE LINEAR REGRESSION) and need to add some feature
"""

file_path = "/content/drive/MyDrive/Toronto_hourly_precipitation_1982_2023 Raw Data (1).csv"
df = pd.read_csv(file_path, parse_dates=['datetime'])

# Drop rows with missing values in critical columns
df = df.dropna(subset=[
    'temp', 'precip', 'humidity', 'dew',
    'windspeed', 'cloudcover', 'visibility', 'solarradiation'
])

# Feature engineering
df['temp_change'] = df['temp'].diff()
df = df.dropna()  # Drop first row with NaN in temp_change

# Define features and target
features = [
    'temp', 'temp_change', 'humidity', 'dew',
    'windspeed', 'cloudcover', 'visibility', 'solarradiation'
]
X = df[features]
y = df['precip']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42,
    stratify=(y > 0).astype(int)  # Handle imbalance
)

"""After defining feature, let's implement MLR and RMSE(Root mean squre error) and R2 for accuracy. According to accuracy matrix, It is shows that model train very well"""

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train MLR model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Predict
y_pred = model.predict(X_test_scaled)

# Evaluation
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
X_test_scaled = scaler.transform(X_test)

# Train MLR model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Predict
y_pred = model.predict(X_test_scaled)

# Evaluation
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

# Print RMSE and R²
print(f"rmse: {rmse:.3f}")
print(f"r2: {r2:.3f}")

rmse
r2

# Visualizations
plt.figure(figsize=(15, 5))

# Plot 1: Temperature vs Precipitation
plt.subplot(1, 2, 1)
plt.scatter(X_test['temp'], y_test, alpha=0.3, label='Actual')
plt.scatter(X_test['temp'], y_pred, alpha=0.3, color='red', label='Predicted')
plt.xlabel('Temperature (°C)')
plt.ylabel('Precipitation (mm)')
plt.title('Precipitation vs Temperature')
plt.legend()

# Plot 2: Temperature Change vs Precipitation
plt.subplot(1, 2, 2)
plt.scatter(X_test['temp_change'], y_test, alpha=0.3, label='Actual')
plt.scatter(X_test['temp_change'], y_pred, alpha=0.3, color='red', label='Predicted')
plt.xlabel('Temperature Change (°C/hour)')
plt.ylabel('Precipitation (mm)')
plt.title('Precipitation vs Temperature Change')
plt.legend()

plt.tight_layout()
plt.show()

# Correlation Analysis
print("\nCorrelation Matrix:")
print(df[['precip', 'temp', 'temp_change']].corr())

# Conditional Analysis (e.g., different behavior when warming vs cooling)
print("\nPrecipitation during warming vs cooling periods:")
print(f"Mean precip when warming (ΔT > 0): {df[df['temp_change'] > 0]['precip'].mean():.4f} mm")
print(f"Mean precip when cooling (ΔT < 0): {df[df['temp_change'] < 0]['precip'].mean():.4f} mm")